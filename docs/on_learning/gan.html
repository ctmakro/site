<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <script>
      (function() {
      var method;
      var methods = [
          'assert', 'clear', 'count', 'debug', 'dir', 'dirxml', 'error',
          'exception', 'group', 'groupCollapsed', 'groupEnd', 'info', 'log',
          'markTimeline', 'profile', 'profileEnd', 'table', 'time', 'timeEnd',
          'timeline', 'timelineEnd', 'timeStamp', 'trace', 'warn'
      ];
      var length = methods.length;
      var console = (window.console = window.console || {});
      while (length--) {
          method = methods[length];
          // Only stub undefined methods.
          if (!console[method]) {
              console[method] = function () {};
          }
      }
      }());
      
      
    </script>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="./../basic.css" rel="stylesheet" type="text/css">
    <link rel="shortcut icon" href="./../favicon.ico">
    <title>GAN 生成式对抗网络</title>
  </head>
  <body id="body">
    <div class="navigation_area">
      <div class="navnode">
              <div class="navnode_title">/</div>
              <div class="navnode"><a href="./../index.html">Index</a>
              </div>
              <div class="navnode"><a href="./../bio.html">About</a>
              </div>
              <div class="navnode">
                      <div class="navnode_title">/lit</div>
                      <div class="navnode"><a href="./../lit/why_bad_idea.html">Why Smart People Have Bad Ideas</a>
                      </div>
              </div>
              <div class="navnode">
                      <div class="navnode_title">/on_learning</div>
                      <div class="navnode"><a href="./../on_learning/gd.html">Gradient Descent 梯度下降法</a>
                      </div>
                      <div class="navnode"><a href="./../on_learning/rl.html">RL is progressing rapidly</a>
                      </div>
                      <div class="navnode"><a href="./../on_learning/resnet_keras.html">Residual Network in Keras</a>
                      </div>
                      <div class="navnode"><a href="./../on_learning/course_resources.html">Learning Resources</a>
                      </div>
                      <div class="navnode"><a href="./../on_learning/fast_gan_in_keras.html"> Fast DCGAN in Keras</a>
                      </div>
                      <div class="navnode"><a href="./../on_learning/gan.html">GAN 生成式对抗网络</a>
                      </div>
                      <div class="navnode"><a href="./../on_learning/artist.html">Training of Artists</a>
                      </div>
                      <div class="navnode"><a href="./../on_learning/how_to.html">Baby steps for ML</a>
                      </div>
                      <div class="navnode">
                              <div class="navnode_title">/on_learning/image</div>
                              <div class="navnode"><a href="./../on_learning/image/style_transfer.html">On Style Transfer 风格转移</a>
                              </div>
                      </div>
                      <div class="navnode">
                              <div class="navnode_title">/on_learning/motor_model</div>
                              <div class="navnode"><a href="./../on_learning/motor_model/motor.html">Learn models of motors</a>
                              </div>
                      </div>
                      <div class="navnode">
                              <div class="navnode_title">/on_learning/audio</div>
                              <div class="navnode"><a href="./../on_learning/audio/wavenet_arch.html"> Behind WaveNet</a>
                              </div>
                      </div>
                      <div class="navnode">
                              <div class="navnode_title">/on_learning/activations</div>
                              <div class="navnode"><a href="./../on_learning/activations/index.html">Activation Functions</a>
                              </div>
                              <div class="navnode"><a href="./../on_learning/activations/relu.html">Rectified Linear Unit 整流线性单元</a>
                              </div>
                      </div>
              </div>
              <div class="navnode">
                      <div class="navnode_title">/on_life</div>
                      <div class="navnode"><a href="./../on_life/electrical.html">Power System Analysis</a>
                      </div>
                      <div class="navnode"><a href="./../on_life/numjs.html">NumJs</a>
                      </div>
              </div>
      </div>
    </div>
    <div class="markdown_content"><h1 data-sourcepos="3:1-3:40">Generative Adversarial Network 生成式对抗网络</h1>
<blockquote data-sourcepos="5:1-5:65">
<p data-sourcepos="5:2-5:65">更新：我用Keras加一点点Tensorflow实现了一个非常快的GAN，详见 &lt;fast_gan_in_keras.html&gt;</p>
</blockquote>
<p data-sourcepos="7:1-7:93">常见的神经网络可以分为“生成式模型”和“区分式模型”。区分式模型(discriminative model)尝试在输入特征之间建立近似的and或者or的关系，以实现对输入进行分类的目的。</p>
<p data-sourcepos="9:1-9:40">比如我们用猫和老虎的脸部图片，训练一个神经网络对物种进行分类，网络通常会这么做：</p>
<img class="graph" src="gan_graph_0.svg" title="graph 0" />
<p data-sourcepos="13:1-13:19">也就是通过条纹的强度，来区分猫和老虎。</p>
<p data-sourcepos="15:1-15:50">如果我们有意增加一些带有条纹的猫的图片，对网络进行训练，为了更明确地区分猫和老虎，网络可能会这么做：</p>
<img class="graph" src="gan_graph_1.svg" title="graph 1" />
<p data-sourcepos="19:1-19:110">我们发现，区分式模型具有“特征惰性(feature laziness)”，也就是神经网络跟人一样善于偷懒，倾向于用最明显、最简单的特征完成分类任务。假如我们现在用这个神经网络对猫和老虎的躯干图片进行分类，准确率会非常低。</p>
<p data-sourcepos="21:1-21:64">为了提高区分式模型的性能，常见的做法就是准备成千上万张不同角度、不同品种的猫和老虎的图片，对网络进行训练。通常会得到这样的结果：</p>
<img class="graph" src="gan_graph_2.svg" title="graph 2" />
<p data-sourcepos="25:1-25:33">通过增加训练数据的量，模型能从数据中学习到更多用于区分物种的特征。</p>
<p data-sourcepos="27:1-27:54">问题是，模型仍然只学到了猫和老虎的各种区别，而无法学习猫和老虎共有的特征（比如都有尾巴，腿都长在躯干下面）。</p>
<p data-sourcepos="29:1-29:95">如果这时我们将这个网络用作识别狮子和豹，准确率会很低，于是又要再准备大量的训练数据。相比之下，人类只需要几张狮子和豹的图片就足够了。这种对人工标记训练数据的无限制的需求，是区分式模型的噩梦。</p>
<h2 data-sourcepos="31:1-31:16">Data Shortage</h2>
<p data-sourcepos="33:1-33:45">正是由于“特征惰性”，很多做机器学习的“数据科学家”们经常会跟其他部门的同事发生如下对话：</p>
<ul data-sourcepos="35:1-43:0">
<li data-sourcepos="35:1-35:15">
<p data-sourcepos="35:3-35:15">请问你们能不能做xyz……</p>
</li>
<li data-sourcepos="36:1-37:0">
<p data-sourcepos="36:3-36:13">数据在哪里？没数据免谈</p>
</li>
<li data-sourcepos="38:1-38:16">
<p data-sourcepos="38:3-38:16">谷歌出了个新的abc功能……</p>
</li>
<li data-sourcepos="39:1-40:0">
<p data-sourcepos="39:3-39:24">那是因为他们积累了足够的用户数据，怎么弄都行</p>
</li>
<li data-sourcepos="41:1-41:12">
<p data-sourcepos="41:3-41:12">项目怎么还没做完……</p>
</li>
<li data-sourcepos="42:1-43:0">
<p data-sourcepos="42:3-42:15">经费不够，不然就请人标注了</p>
</li>
</ul>
<h2 data-sourcepos="44:1-44:43">Unsupervised Learning came to be rescued</h2>
<p data-sourcepos="46:1-46:107">自然界中带有明确分类标签的数据是非常稀少的，特别是这些标签只对人类有意义，因此只能由人类来进行标示，产生了很大的工作量。然而人类在了解世界的过程中，并不需要将一切所见所闻都打上标签，那么人类是怎样实现高效学习的呢？</p>
<p data-sourcepos="48:1-48:93">神经科学理论认为，人类学习源自对周围环境的预测。如果身边的一切事物跟预想的一样规律运作，学习就不会发生；如果所预测的运作与观测不符，学习就会发生。这种能力能够帮助我们逐渐适应并利用环境。</p>
<p data-sourcepos="50:1-50:67">回到对猫、老虎、狮子、豹进行分类的例子上来。人类通过大量观察各种不同的动物（河马、大象、狗、驴等等），可以在大脑中建立这样的神经网络：</p>
<img class="graph" src="gan_graph_3.svg" title="graph 3" />
<p data-sourcepos="54:1-54:92">即便这些训练数据都不带标签，人类也能从中学到一些关于动物的知识，比如动物都是一个躯干旁边四条腿，如果在躯干四周看不到腿，或者看不到尾巴，很可能就不是动物(图中的not_animal)。</p>
<p data-sourcepos="56:1-56:182">在掌握“腿”和“躯干”之间的互相依赖关系这样的抽象特征之后，再去对猫、老虎、狮子、豹进行分类，人类就能够将这些概念与图中的具体动物进行匹配，从而减少需要学习的特征数量，降低学习难度。例如，我们看到图片中的毛发，就可以根据毛发位于躯干四周的具体位置，来决定毛发究竟是属于躯干，还是属于腿部、尾部、耳朵，而不需要再从数据中专门学习“腿部毛发”、“尾部毛发”这样的概念。</p>
<img class="graph" src="gan_graph_4.svg" title="graph 4" />
<p data-sourcepos="60:1-60:110">结果就是，对于新的分类任务，只需要使用很少的带标签数据，就能训练出非常高效的分类器。历史上有一位长者常说他“见的多了”，“哪个国家没去过”，不管是跟找茬的美国记者还是跟赛跑的香港记者一样能“谈笑风生”，其实是一样的原理。</p>
<p data-sourcepos="62:1-62:121">在ML领域，这类对不带标签的数据进行学习并掌握概括性特征的方法，被归类为Unsupervised Learning（非监督式学习）。由于这方面商业兴趣不大，所以应用一直滞后于Supervised Learning(监督式学习，或带标签学习)。</p>
<h2 data-sourcepos="64:1-64:6">GAN</h2>
<p data-sourcepos="66:1-66:92">可以这么说：监督式学习是用神经网络拟合 p(标签|数据)，而非监督式学习是用神经网络拟合 p(数据)，即从数据中寻找具有代表性、概括性的特征（或者说，适应数据特征之间的条件概率分布）。</p>
<p data-sourcepos="68:1-68:56">在没有标签的情况下，我们无法通过标签提供的误差梯度对网络进行训练，必须要通过其他方法定义误差函数以产生误差梯度。</p>
<p data-sourcepos="70:1-70:89">这方面的先驱是Hinton提出的Restricted Boltzmann Machine；其他方法还包括 Variational Auto Encoder等等。在此略过不做介绍。</p>
<p data-sourcepos="72:1-72:52">真正让人眼前一亮的，是最近搞起的Generative Adversarial Network，即GAN。</p>
<p data-sourcepos="74:1-74:30">GAN方法需要两个神经网络，一个生成网络G和一个区分网络D。</p>
<img class="graph" src="gan_graph_5.svg" title="graph 5" />
<p data-sourcepos="78:1-79:29">生成网络G的输入是一个矢量，输出一张图片；
区分网络D的输入是一张图片，输出一个(0,1)之间的概率。</p>
<p data-sourcepos="81:1-81:71">区分网络D的任务是辨别输入的真伪。如果输入图片来自训练样本(x)，则应该输出接近1的概率：如果输入图片来自生成网络G，则应该输出接近0的概率。</p>
<blockquote data-sourcepos="83:1-83:28">
<p data-sourcepos="83:3-83:28">即： D(G(z)) -&gt; 0， D(x) -&gt; 1</p>
</blockquote>
<p data-sourcepos="85:1-85:6">首先训练D:</p>
<blockquote data-sourcepos="87:1-87:40">
<p data-sourcepos="87:3-87:40">maximize log(D(x)) + log(1-D(G(随机矢量)))</p>
</blockquote>
<p data-sourcepos="89:1-89:21">这会提高D对训练样本和生成样本的区分能力。</p>
<p data-sourcepos="91:1-91:6">然后训练G:</p>
<blockquote data-sourcepos="93:1-93:26">
<p data-sourcepos="93:3-93:26">maximize log(D(G(随机矢量)))</p>
</blockquote>
<p data-sourcepos="95:1-95:17">这会提高G生成样本对D的欺骗能力。</p>
<p data-sourcepos="97:1-97:117">如此循环下去，G会生成在特征上越来越接近训练样本的图片；而D会对训练样本和生成样本之间的差异（也就是训练样本不同特征之间的相互依赖关系）越来越敏感。于是很多人将GAN中的G用作图像生成、图像修补、图像超采样等领域，获得了非常好的效果。</p>
<p data-sourcepos="99:1-99:71">我根据DCGAN的Torch源码，利用Keras实现了一个GAN。训练样例是CIFAR-10中央16x16。以下是经过训练之后，G生成的样本：</p>
<p data-sourcepos="101:1-103:20"><img src="gan_cifar_1.png" alt="" />
<img src="gan_cifar_2.png" alt="" />
<img src="gan_cifar_3.png" alt="" /></p>
<p data-sourcepos="105:1-105:158">从远处来看，这些样本非常像真实的自然照片，近看才发现其实什么也不像。这说明G网络已经大致掌握了训练样例各个像素之间的条件概率分布，例如背景前景颜色的分布，动植物特征的分布等等。这同时也说明D网络现在已经掌握了训练样例中的这些特征（并能够利用这些特征区分真假图片）。而整个训练过程并不需要提供与图像内容相关的任何标签。</p>
<p data-sourcepos="107:1-107:73">很多GAN相关的paper也提到：在大量无标签数据上用GAN方法进行训练之后，将D网络用于分类，只需要很少标签就可以在短时间内达到非常高的准确率。</p>
<h2 data-sourcepos="109:1-109:19">Network Capacity</h2>
<p data-sourcepos="111:1-111:104">由于GAN的拟合对象是p(data)而不是p(label|data)，即data中与label无关的部分也得拟合，因此需要的网络容量会大大超过对data按label分类的分类器网络，不然生成的样本质量会很差。</p>
<p data-sourcepos="113:1-113:44">这其实就类似于：一位优秀的艺术家和一位普通人经过一个房间，要求他们用笔画出房间里的内容。</p>
<ul data-sourcepos="115:1-117:0">
<li data-sourcepos="115:1-115:38">优秀的艺术家（生成式模型）可以在纸上还原房间的所有细节（不管是否重要）；</li>
<li data-sourcepos="116:1-117:0">而一般人（区分式模型）只能在脑海里还原很少一部分、对完成日常任务有帮助的细节。</li>
</ul>
<p data-sourcepos="118:1-118:69">这也可以解释为什么很多同学的英语填空选择很高分，但是作文和口语却很差：前者是区分式模型，后者是生成式模型，所需的训练量和容量差两个数量级。</p>
<h2 data-sourcepos="120:1-120:22">GAN Training Tricks</h2>
<p data-sourcepos="122:1-122:70">在所有的tricks里面，亲测最有效的是batch discrimination和single-sided label smoothing.</p>
<p data-sourcepos="124:1-124:83">G网络有一个取胜策略：令生成的每张图像都长得很像训练样例中的某（几）张图片，简称mode collapse。后果就是每次G网络生成的结果全都一样，D网络却分辨不出来。</p>
<p data-sourcepos="126:1-126:103">解决方法:在D网络中的每一层，计算G网络生成的图像之间的相似度，并把相似度作为feature map输入到下一层，这样D网络就可以利用相似度信息识别出G网络生成的相似图片，以鼓励G网络生成具有多样性的图片。</p>
<ul data-sourcepos="128:1-232:0">
<li data-sourcepos="128:1-174:0">
<p data-sourcepos="128:3-128:22">Batch Discrimination</p>
<p data-sourcepos="130:3-130:161">在网络的每一层，对于每个minibatch中的每个sample，求其所有feature maps到其他sample的feature maps的距离，并将此距离作为一个feature map合并到其feature maps中。不同的paper对此有不同的实现方式，我的实现方式很简单，见下面的concat_diff函数：</p>
<pre><code data-sourcepos="132:3-173:17" class="language-py"># Keras w/ TensorFlow
def concat_diff(i): # i -&gt; Tensor(batch,height,width,features)
    bv = Lambda(
    lambda x:K.mean(K.abs(x[:] - K.mean(x,axis=0)),axis=-1,keepdims=True)
    )(i)
    i = merge([i,bv],mode='concat')
    return i

def dis2(): # discriminative network, 2
    inp = Input(shape=(16,16,3))
    i = inp

    ndf=16

    def conv(i,nop,kw,std=1,usebn=True):
        i = Convolution2D(nop,kw,kw,border_mode='same',subsample=(std,std))(i)
        if usebn:
            i = bn(i) # batch normalization
        i = relu(i)
        return i

    i = conv(i,ndf,4,std=1,usebn=False)
    i = concat_diff(i)
    i = conv(i,ndf*2,4,std=2)
    i = concat_diff(i)
    i = conv(i,ndf*4,4,std=2)
    i = concat_diff(i)
    i = conv(i,ndf*8,4,std=2)
    i = concat_diff(i)
    i = Convolution2D(1,2,2,border_mode='valid')(i)

    i = Activation('linear',name='conv_exit')(i)
    i = Activation('sigmoid')(i)

    i = Reshape((1,))(i)

    m = Model(input=inp,output=i)
    mf = Model(input=inp,output=i)
    mf.trainable = False
    return m,mf
</code></pre>
</li>
<li data-sourcepos="175:1-194:0">
<p data-sourcepos="175:3-175:30">Single-sided Label Smoothing</p>
<p data-sourcepos="177:3-177:22">这是D原本的cost function:</p>
<pre><code data-sourcepos="179:3-182:32" class="language-py">loss = - (log(1-D(G(z))) + log(D(x)))
# without s-side lbl smoothing
</code></pre>
<p data-sourcepos="184:3-184:57">麻烦的是，如果D过于占上风（将训练样例和生成图片分得太清楚），G的参数梯度就会过大，有可能导致G网络无法收敛。</p>
<p data-sourcepos="186:3-186:62">因此在计算binary cross-entropy函数的时候，可以将训练样例的概率目标调低一点，比如从1.0调到0.9：</p>
<pre><code data-sourcepos="188:3-191:29" class="language-py">loss = - (log(1-D(G(z))) + 0.1 * log(1-D(x)) + 0.9 * log(D(x)))
# with s-side lbl smoothing
</code></pre>
<p data-sourcepos="193:3-193:31">由于只修改了训练样例的概率目标，这种方法称为单边标签平滑。</p>
</li>
<li data-sourcepos="195:1-232:0">
<p data-sourcepos="195:3-195:38">My Generative Network implementation</p>
<pre><code data-sourcepos="197:3-231:17" class="language-py"># Keras w/ TensorFlow
def gen2(): # generative network, 2
    inp = Input(shape=(zed,))
    i = inp
    i = Reshape((1,1,zed))(i)

    ngf=16

    def deconv(i,nop,kw,oh,ow,std=1,tail=True,bm='same'):
        global batch_size
        i = Deconvolution2D(nop,kw,kw,
        subsample=(std,std),
        border_mode=bm,
        output_shape=(batch_size,oh,ow,nop)
        )(i)
        if tail:
            i = bn(i) # batch normalization
            i = relu(i)
        return i

    i = deconv(i,nop=ngf*8,kw=2,oh=2,ow=2,std=1,bm='valid')
    i = deconv(i,nop=ngf*4,kw=4,oh=4,ow=4,std=2)
    i = deconv(i,nop=ngf*2,kw=4,oh=8,ow=8,std=2)
    i = deconv(i,nop=ngf*1,kw=4,oh=16,ow=16,std=2,tail=False)
    i = relu(i)

    i = deconv(i,nop=3,kw=4,oh=16,ow=16,std=1,tail=False)
    i = Activation('tanh')(i)

    m = Model(input=inp,output=i)
    mf = Model(input=inp,output=i)
    mf.trainable = False
    return m,mf
</code></pre>
</li>
</ul>
<h2 data-sourcepos="233:1-233:5">最后</h2>
<p data-sourcepos="235:1-235:54">推荐看Ian Goodfellow对GAN的专题视频。目前效果最好的2D图像生成式模型是PPGN，不妨关注。</p>
</div>
    <div class="meta_string">
      <p>file: gan.md</p>
      <p>last modified: 2017-02-07 23:54</p>
    </div>
    <script>
      //if(window.HighlightEverything){window.HighlightEverything()}
      
    </script>
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/atom-one-light.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>
    <script>
      hljs.initHighlightingOnLoad();
      
    </script>
    <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
  </body>
</html>